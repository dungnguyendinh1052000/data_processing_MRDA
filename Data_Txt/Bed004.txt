__z__okay .__c1
__z__oh i don't ==__c2
____i think i'm zero .__c0
__z__wow !__c1
__z__uh ==__c4
__z__wh- - what causes the crash ?__c9
__z__unprecedented .__c1
____hello hello hello hello .__c2
____did you fix something ?__c0
__z__hello .__c2
__z__five five .__c4
__z__hello hello .__c2
__z__oh maybe it's the turning - turning off and turning on of the mike .__c9
__z__right ?__c9
__z__uh - you think that's you ?__c1
__z__oh .__c1
__z__ah-ah-ah ==__c2
__z__yeah okay mine's working .__c9
__z__okay .__c2
__z__that's me .__c2
__z__okay .__c1
__z__okay .__c1
__z__so um - i guess we are um - going to do the digits at the end .__c1
__z__uh ==__c1
__z__channel - channel three ?__c3
__z__yeah .__c3
__z__huh channel five ?__c4
__z__okay .__c3
__z__channel two .__c2
__z__doesn't work ?__c4
__z__two .__c2
__z__yeah that's the mike number there uh - uh - mike number five and channel - channel four .__c1
__z__no uh ==__c4
__z__is it written on her sheet i believe .__c0
__z__mike four .__c3
__z__watch this .__c9
__z__era cuatro .__c4
__z__yep that's me .__c9
__z__yeah .__c4
__z__but channel ==__c0
__z__yeah yeah yeah .__c4
__z__this is you .__c1
__z__okay .__c4
__z__i saw that .__c4
__z__uh - yeah it's okay .__c4
__z__yeah .__c1
__z__and i'm channel uh - i think .__c1
__z__ooo .__c2
__z__or channel ==__c1
__z__i think i'm channel two .__c2
__z__oh i'm channel - must be channel one .__c1
__z__channel - i decided to talk about that .__c4
__z__channel one ?__c1
__z__yes okay .__c1
__z__okay .__c1
__z__so uh - i also copied uh - the results that we all got in the mail i think from uh - from o g i .__c1
__z__and we'll go - go through them also .__c1
__z__so where are we on - on uh - our runs ?__c1
__z__uh - uh - we - so | as i was already said we - we mainly focused on uh - four kind of features .__c3
__z__excuse me .__c1
__z__the p l p the p l p with j rasta the m s g and the m f c c from the baseline aurora .__c3
__z__uhhuh .__c1
__z__uh - and we focused for the - the test part on the english and the italian .__c3
__z__um - | we've trained uh - several neural networks on ==__c3
__z__so - on the t i digits english and on the italian data .__c3
__z__and also on the broad uh - english uh - french and uh - spanish databases .__c3
__z__huh - so there's our result tables here for the tandem approach .__c3
__z__and um - actually what we - we @reject@ observed is that if the network is trained on the task data it works pretty well .__c3
__z__okay .__c1
__z__chicken on the grill .__c2
__z__our - our uh - there's a - we're pausing for a photo .__c1
__z__try that corner .__c2
__z__how about over - th- - from the front of the room ?__c0
__z__yeah it's longer .__c2
__z__we're pausing for a photo opportunity here .__c1
__z__uh - uh ==__c1
__z__so ==__c1
__z__oh wait wait wait wait wait wait .__c9
__z__get out of the ==__c2
__z__yeah .__c2
__fh__hold on .__c9
__b__hold on .__c9
__s^co^t__okay .__c1
__%--__let me give you a black screen .__c9
__qy__he's facing this way .__c1
__qy__what ?__c1
__s^ft__okay this - this would be a good section for our silence detection .__c1
__b__okay .__c9
__fh|s__uhhuh .__c2
__s^rt__um - oh .__c1
__s^rt__musical chairs everybody .__c9
__s^rt__okay | so um - you were saying about the training data .__c1
__b^rt__yeah .__c3
__s^rt__yeah .__c1
__s__so if the network is trained on the task data um - tandem works pretty well .__c3
__s__and uh - actually we have uh ==__c3
__%-__results are similar only on ==__c3
__qy^bu^d^rt__do you mean if it's trained only on - on data from just that task ?__c0
__s^na^rt.%--__yeah .__c3
__s^rt__that language ?__c0
__fh|s__just that task .__c3
__s.%--__but actually we didn't train network on uh - both types of data .__c3
__s^rt__i mean - uh - phonetically ba- ?==__c3
__fh__phonetically balanced uh - data and task data .__c3
__s__we only did either task - task data or uh - broad data .__c3
__fh__huh .__c0
__s.%--__uhhuh .__c0
__s^t__um - yeah .__c3
__fh__so ==__c3
__fh__so how ?==__c1
__s^rt__i mean - clearly it's going to be good then .__c1
__s.%--__so what's th- ?==__c0
__s.%--__but the question is how much worse is it if you have broad data ?__c1
__fg|s__i mean my assump- ==__c1
__fh|s__from what i saw from the earlier results uh - i guess last week was that um - if you trained on one language and tested on another say that the results were - were relatively poor .__c1
__qy^d^g^rt__huh - yeah .__c3
__fh|s__but - but the question is if you train on one language but you have a broad coverage and then test in another does that - is that improve things i- - c- - in comparison ?__c1
__qy^bu^d^rt__if we use the same language ?__c3
__s^aa__no no no .__c1
__s__different lang- ==__c1
__s^bk__so um - if you train on t i digits and test on italian digits you do poorly let's say .__c1
__s__uhhuh .__c3
__s__i don't have the numbers in front of me .__c1
__s^bk__but - yeah but i did not uh - do that .__c3
__s.%--__so i'm just imagining .__c1
__%--__e- - so you didn't train on timit and test on - on italian digits say ?__c1
__s.%--__we no | we did four - four kind of of testing actually .__c3
__fh|qy^rt__the first testing is with task data .__c3
__s^ar__so with nets trained on task data .__c3
__s^co__so for italian on the italian speech @reject@ .__c3
__sj^ba.%--__the second test is trained on a single language um - with broad database .__c3
__s^co__but the same language as the t- - task data .__c3
__s__okay .__c1
__s^bk__but for italian we choose spanish which we assume is close to italian .__c3
__fh|s^rt__the third test is by using um - the three language database .__c3
__s^df__and the fourth is ==__c3
__s__w- - which in ?==__c1
__qy^rt__it has three languages .__c1
__qy__that's including the - w- - the - the ?==__c1
__s__this includes ==__c3
__%--__the one that it's ==__c1
__s^bk__yeah .__c3
__s__but not digits .__c3
__qy^rt__in- ==__c0
__fh|s__i mean - it's ==__c3
__s.%--__right .__c1
__qy^rt__the three languages is not digits .__c0
__s^rt__it's the broad data .__c0
__s__yeah .__c3
__s__okay .__c0
__s^t^tc__and the fourth test is uh - excluding from these three languages the language that is the task language .__c3
__s^cs__oh okay .__c1
__sj__yeah so that is what i wanted to know .__c1
__fh|s__yeah .__c3
__s__i just wasn't saying it very well i guess .__c1
__%-__uh - yeah .__c3
__qh__so | um - for uh - t i digits for ins- ==__c3
__s^ar__example uh - when we go from t i digits training to timit training uh - we lose uh - around ten per cent .__c3
__s^rt__uh - the error rate increase u- - of - of - of ten per cent relative .__c3
__sj__relative .__c1
__s^bu.%--__right .__c1
__s^aa|s.%--__so this is not so bad .__c3
__s^bk__and then when we jump to the multilingual data it's - uh - it become worse .__c3
__s^bk__and well around uh - let's say twenty perc- - twenty per cent further .__c3
__fh|s^rt__ab- - about how much ?__c1
__fh|s^cs^rt__so ==__c3
__s__yeah .__c3
__b^rt__twenty per cent further ?__c1
__s^aa__twenty to - to thirty per cent further .__c3
__fg|s^cs__yeah .__c3
__s^e^rt__and so remind me .__c0
__b^rt__the multilingual stuff is just the broad data .__c0
__s^rt__right ?__c0
__fh|s^cs__yeah .__c3
__s__it's not the digits .__c0
__s__so it's the combination of two things there .__c0
__qy^2^rt__it's removing the task specific training and it's adding other languages .__c0
__b__yeah yeah .__c3
__s^cs^rt__okay .__c0
__b__but the first step is al- - already removing the task s- - specific from - from ==__c3
__s^rt__already .__c0
__qw__right right right .__c0
__fh|s^rt__so ==__c3
__b^rt__and we lose ==__c3
__fh|s^rt__so they were sort of building here .__c0
__fh__yeah .__c3
__s^aa__okay .__c0
__s__uh - | so basically when it's trained on the - the multilingual broad data um - or number .__c3
__fg__so ==__c3
__s.%--__the - the ratio of uh- ==__c3
__qy^bu^rt__our error rates uh - with the baseline error rate is around uh - one point one .__c3
__s^no__so ==__c3
__s^no__yes | and it's something like one point three of - of the uh ==__c1
__b__i- - i- - if you compare everything to the first case at the baseline .__c1
__s__you get something like one point one for the - for the using the same language but a different task .__c1
__fh__and something like one point three for three - three languages broad stuff .__c1
__s^aa|s.%--__no no no .__c3
__s.%-__uh - same language we are at uh - for at english at o point eight .__c3
__b__so it improves compared to the baseline .__c3
__b__but ==__c3
__fg__so ==__c3
__fg__le- - let me ==__c3
__fg__i - i - i'm sorry .__c1
__fh|s.%--__tas- - task data ==__c3
__s__we are u- ==__c3
__b__i - i - i meant something different by baseline .__c1
__fg__yeah .__c3
__fg__so let me - let me ==__c1
__fh|s^t__um - so um ==__c1
__s__huh .__c3
__fh|qy__okay fine .__c1
__qrr.%--__let's - let's use the conventional meaning of baseline .__c1
__qw__huh .__c3
__s^ng__i - i - by baseline here i meant uh - using the task specific data .__c1
__s^bk__oh yeah the f- - yeah okay .__c3
__s^co.%--__yeah .__c3
__fh__but uh - uh - because that's what you were just doing with this ten per cent .__c1
__fh__so i was just - i just trying to understand that .__c1
__%__yeah .__c3
__fh__sure .__c3
__qh^co__so if we call a factor of w- - just one - just normalized to one the word error rate that you have for using t i digits as - as training and t i digits as test ==__c1
__s.%-__huh .__c3
__s^2__uhhuh .__c3
__%-__uh - different words i'm sure .__c1
__fh__but - but uh - uh - the same task and so on .__c1
__s^ng__uhhuh .__c3
__s__if we call that one then what you're saying is that the word error rate for the same language but using uh - different training data than you're testing on say timit and so forth it's one point one .__c1
__fh__uhhuh .__c3
__b__yeah it's around one point one .__c3
__%--__right .__c1
__s^tc__and if it's ==__c1
__sj__yeah .__c3
__s__you do - go to three languages including the english it's something like one point three .__c1
__s__ye- ==__c3
__b__that's what you were just saying i think .__c1
__sj^ba__uh - more actually .__c3
__s__one point four ?__c0
__b__if i - yeah .__c3
__s__so it's an additional thirty per cent ?__c0
__s^rt__what would you say ?__c3
__b__around one point four .__c3
__s__yeah .__c3
__fh|sj^cs__okay .__c1
__s^aa__and if you exclude english from this combination what's that ?__c1
__s__if we exclude english um - there is not much difference with the data .__c3
__%__with english .__c3
__sj^ba__so yeah .__c3
__fh|s__aha !__c1
__b__that's interesting that's interesting .__c1
__fh|s__do you see because uh ?==__c1
__sj__uh ==__c3
__b__so - no that - that's important .__c1
__sj__so what - what it's saying here is just that ==__c1
__fh__yes .__c1
__b__there is a reduction in performance when you don't um - have the s- - when you don't have um ==__c1
__s__task data .__c0
__fh__wait a minute .__c1
__sj^ba__th- - th- - the ==__c1
__fh|s__huh .__c3
__s__no actually | it's interesting .__c1
__fh__so it's - so when you go to a different task there's actually not so different .__c1
__s.%-__it's when you went to these ==__c1
__s.%-__so what's the difference between two and three ?__c1
__b__between the one point one case and the one point four case ?__c1
__fg__i'm confused .__c1
__s^ng__it's multilingual .__c0
__fh|s__yeah .__c3
__fg__the only difference it's - is that it's multilingual .__c3
__s^cs^rt__um ==__c3
__b^rt__because in both - in both - both of those cases you don't have the same task .__c1
__b__yeah .__c3
__fh|s^e__yeah sure .__c3
__fh|s__so is - is the training data for the for this one point four case ?==__c1
__b__does it include the training data for the one point one case ?__c1
__b__uh - | yeah .__c3
__fh|sj^ba^tc__yeah | a fraction of it .__c9
__fh|s^cs^t__a part of it .__c3
__b^rt__yeah .__c3
__fh__how m- - how much bigger is it ?__c1
__s__um - | it's two times .__c3
__s__yeah um ==__c9
__s__actually ==__c3
__sj^ba__yeah .__c3
__b__um - | the english data - no the multilingual databases are two times the broad english data .__c3
__s__we just wanted to keep this w- - well not too huge .__c3
__s^cs__so ==__c3
__fh|s^bc__so it's two times .__c1
__s__but it includes the - but it includes the broad english data .__c1
__b__i think so .__c3
__fh|s__do you ?==__c3
__s^t.%--__uh - | yeah .__c3
__sj__and the broad english data is what you got this one point one with .__c1
__s__so that's timit basically .__c1
__s^co__right ?__c1
__fh|s__yeah .__c3
__s__uhhuh .__c9
__b__so it's band limited timit .__c1
__qy^rt__uhhuh .__c9
__fh.%__uhhuh .__c3
__s^rt__this is all eight kilohertz sampling .__c1
__sj.%--__yeah .__c3
__s__downs- ==__c9
__s__right .__c9
__fh|s__so you have band limited timit gave you uh - as good as a result as using t i digits on a t i digits test .__c1
__b__okay .__c1
__sj^ba__huh .__c3
__s^t.%--__um - and um ==__c1
__s^cs^rt^t__but when you add in more training data but keep the neural net the same size it um - performs worse on the t i digits .__c1
__sj^ba__okay now all of this is this is noisy t i digits i assume .__c1
__s^rt^t__yup .__c3
__s^e^rt^t__both training and test ?__c1
__fh|s__yeah .__c1
__s__okay .__c1
__s^df__um ==__c1
__s.%--__okay | well we - we - we may just need to uh ==__c1
__s__so i mean - it's interesting that h- - going to a different - different task didn't seem to hurt us that much .__c1
__b__and going to a different language um ==__c1
__s__it doesn't seem to matter ==__c1
__b__the difference between three and four is not particularly great .__c1
__s__so that means that whether you have the language in or not is not such a big deal .__c1
__s__huh .__c3
__fh|s__it sounds like um - uh - we may need to have more of uh - things that are similar to a target language .__c1
__b^rt__or i mean - you have the same number of parameters in the neural net .__c1
__s__you haven't increased the size of the neural net .__c1
__fh__and maybe there's just - just not enough complexity to it to represent the veri- - variab- - increased variability in the - in the training set .__c1
__s^co.%--__that - that could be ==__c1
__s^no__um - | so what about ?==__c1
__s^co.%--__so these are results with uh - th- - that you're describing now that they are pretty similar for the different features or - or uh ?==__c1
__s^co__uh - | let me check .__c3
__qy^rt__uh ==__c3
__s^aa|s.%-__yeah .__c1
__b__so this was for the p l p .__c3
__s^bk__yeah .__c1
__s^bk__um - the ==__c3
__fh__yeah .__c3
__s^rt__for the p l p with j rasta the - the - we ==__c3
__b^rt__this is quite the same tendency with a slight increase of the error rate uh - if we go to - to timit .__c3
__s__and then it's - it gets worse with the multilingual .__c3
__s__um - | yeah .__c3
__b__there - there is a difference actually with - b- - between p l p and j rasta .__c3
__b^rt__is that j rasta seems to perform better with the highly mismatched condition but slightly - slightly worse for the well matched condition .__c3
__s__huh .__c3
__s^aa__i have a suggestion actually .__c1
__s^aa__even though it'll delay us slightly .__c1
__h|s__would - would you mind running into the other room and making copies of this ?__c1
__b__because we're all sort of ==__c1
__s__yeah yeah .__c3
__s__if we - c- - if we could look at it while we're talking i think it'd be ==__c1
__fh|s.%--__okay .__c3
__s^bc__uh - uh i'll - i'll sing a song or dance or something while you do it too .__c1
__fh|s.%--__all right .__c9
__b__so um ==__c0
__s^bc__go ahead .__c0
__s__uh - while you're gone i'll ask s- - some of my questions .__c0
__s__yeah .__c1
__qy^d^g^rt__um ==__c0
__s__yeah .__c1
__b^rt__uh - this way and just slightly to the left .__c1
__s^e__yeah .__c1
__fh|s__the um - what was - was this number forty ?__c0
__s__or - it was roughly the same as this one he said ?__c0
__s__um ==__c1
__fh|s.%--__when you had the two language versus the three language .__c0
__s__that's what he was saying .__c1
__fh|s__that's where he removed english .__c0
__fh|s__yeah .__c9
__s__right ?__c0
__b^rt__right .__c1
__fh|s^rt__it sometimes actually depends on what features you're using .__c9
__s__yeah .__c1
__s^bk__but - but i- - it sounds like ==__c1
__fh|s__um - but - he - uhhuh .__c9
__s__i mean - that's interesting .__c1
__b^rt__because it - it seems like what it's saying is not so much that you got hurt uh - because you uh - didn't have so much representation of english .__c1
__fh|s__because in the other case you don't get hurt any more .__c1
__qw__at least when it seemed like uh - it - it might simply be a case that you have something that is just much more diverse .__c1
__s^m.%--__uhhuh .__c0
__s__but you have the same number of parameters representing it .__c1
__b^rt__uhhuh .__c0
__s^e__i wonder .__c0
__h__were um - all three of these nets using the same output ?__c0
__s^tc__this multi language uh - labelling labelling ?__c0
__s__he was using uh - sixty four phonemes from sampa .__c9
__s^no__okay okay .__c0
__s__yeah .__c9
__%-__so this would - from this you would say well it doesn't really matter if we put finnish into the training of the neural net if there's going to be you know - finnish in the test data .__c0
__s__right ?__c0
__s^aa__well it's - it sounds - i mean - we have to be careful .__c1
__s^arp__because we haven't gotten a good result yet .__c1
__fh__yeah .__c0
__qw.%--__and comparing different bad results can be tricky .__c1
__s^co__huh .__c0
__%--__but i - i - i - i think it does suggest that it's not so much uh - uh - cross language as cross type of speech .__c1
__s__uhhuh .__c0
__%--__it's - it's uh ==__c1
__b__um - but we did ==__c1
__b__oh yeah .__c1
__fg|s^cs.%--__the other thing i was asking him though is that i think that in the case ==__c1
__fh|s^co__yeah .__c1
__s^nd.%--__you - you do have to be careful .__c1
__s^co__because of com- - compounded results .__c1
__s^co__i think we got some earlier results in which you trained on one language and tested on another .__c1
__s^co__and you didn't have three but you just had one language .__c1
__s^aa__so you trained on one type of digits and tested on another .__c1
__s^co^r__didn- - wasn't there something of that ?__c1
__s^nd.%-__where you say you trained on spanish and tested on - on t i digits ?__c1
__s^bk__or the other way around ?__c1
__fg|s__something like that ?__c1
__s^e.%--__no .__c4
__s.%--__i thought there was something like that that he showed me last week .__c1
__s^bu__we'll have to wait till he gets back .__c1
__s^ng.%--__until we get ==__c1
__s__yeah that would be interesting .__c0
__fh__um - this may have been what i was asking before stephane .__c1
__s^df__but - but um - wasn't there something that you did where you trained on one language and tested on another ?__c1
__s__i mean - no - no mixture but just ==__c1
__s__i'll get it for you .__c9
__s^cs__uh - no no .__c3
__fh__we've never just trained on one lang- ?==__c1
__s^na__training on a single language you mean ?__c3
__s__and testing on the other one ?__c3
__s__yeah .__c1
__s^bk__not yet .__c4
__fg__uh - no .__c3
__fh__so the only task that's similar to this is the training on two languages .__c3
__s__and that ==__c3
__fh|s^co^t__but we've done a bunch of things where we just trained on one language .__c1
__s^e__right ?__c1
__s__i mean - you haven't - you haven't done all your tests on multiple languages .__c1
__fh__uh - | no .__c3
__s__either thi- - this is test with uh - the same language but from the broad data .__c3
__b__or it's test with uh - different languages .__c3
__fh|s.%--__also from the broad data .__c3
__s^no__excluding the ==__c3
__s__the early experiment that ==__c4
__s^bu.%-__so it's - it's three or - three and four .__c3
__s^aa__did you do different languages from digits ?__c0
__s^bk__uh - | no .__c3
__b__you mean training digits on one language and using the net to recognize on the other ?__c3
__fh__digits on another language .__c0
__fh__no .__c3
__s^bu.%--__see i thought you showed me something like that last week .__c1
__s__you had a - you had a little ==__c1
__s.%--__uh - | no i don't think so .__c3
__s.%-__um - | what ?==__c1
__fg__these numbers are uh - ratio to baseline ?__c2
__s^cs.%--__so i mean - wha- - what's the ?==__c1
__b__so ==__c3
__s^cs^e__this - this chart - this table that we're looking at is um - show- - is all testing for t i digits ?__c1
__fh|s__or ?==__c1
__s__bigger is worse .__c9
__b__so you have uh - basically two uh - parts .__c3
__s__this is error rate i think .__c9
__fh|s__ratio .__c2
__qy^d^f^g^rt__no no .__c9
__s__the upper part is for t i digits .__c3
__qy^d^f^g^rt__yeah yeah yeah .__c9
__s^cs__and it's divided in three rows of four - four rows each .__c3
__b__uhhuh .__c9
__s__yeah .__c1
__s^cs__and the first four rows is well matched .__c3
__s^bk__then the s- - the second group of four rows is mismatched .__c3
__s__and finally highly mismatched .__c3
__fh__and then the lower part is for italian .__c3
__fh__and it's the same - the same thing .__c3
__qy^bu^d^rt__so - so the upper part is training t i digits ?__c0
__%__so it's - it's the h t k results i mean .__c3
__b__so it's h t k training testings with different kind of features .__c3
__%-__uh .__c0
__h|s^aap__and what appears in the uh - left column is the networks that are used for doing this .__c3
__fh|s__huh .__c1
__s__so uh - yeah .__c3
__b__well what was - is that i- - what was it that you had done last week when you showed ?==__c1
__fh|s.%--__do you remember ?__c1
__s__wh- - when you showed me the - your table last week .__c1
__b__it - it was part of these results .__c3
__h|s.%--__huh .__c3
__s^bc__huh .__c3
__s^co__so where is the baseline for the t i digits located in here ?__c0
__fh__you mean the h t k aurora baseline ?__c3
__fh__yeah .__c0
__sj__it's uh - the one hundred number .__c3
__s__it's well all these numbers are the ratio with respect to the baseline .__c3
__s^cs__uh !__c0
__b__uh - okay okay .__c0
__sj__so this is word - word error rate .__c1
__%-__so a high number is bad .__c1
__fg|s^co__yeah this is a word error rate ratio .__c3
__fh|s__so it's - yeah .__c4
__fg|s.%--__yeah .__c3
__s__okay i see .__c0
__s.%--__so seventy point two means that we reduced the error rate uh - by thirty - thirty per cent .__c3
__s__so ==__c3
__s__okay okay gotcha .__c0
__s^bc__huh .__c3
__s^df__okay so if we take ==__c1
__s^t1__uh um ==__c1
__s__let's see .__c1
__fh__p l p uh - with online normalization and delta l del- ==__c1
__s__so that's this thing you have circled here in the second column .__c1
__s__yeah .__c3
__b^rt__um - and multi english refers to what ?__c1
__s.%--__to timit .__c3
__s__huh .__c3
__fh|s^cc__then you have uh - m f m s and m e .__c3
__b^rt__which are for french spanish and english .__c3
__b__and ==__c3
__s__yeah .__c3
__s__actually i - i uh - forgot to say that the multilingual net are trained on uh - features without the s- - derivatives .__c3
__s^bs__uh - but with increased frame numbers .__c3
__s__huh .__c3
__s.%--__and we can - we can see on the first line of the table that it - it - it's slightly - slightly worse when we don't use data .__c3
__s__delta but it's not - not that much .__c3
__fh|s__right .__c1
__fg__so w- - w- - so i'm sorry i missed that .__c1
__%--__what's m f m s and m e ?__c1
__qw__multi french .__c0
__s__so multi french multi spanish and multi english .__c3
__s^co__multi spanish .__c0
__fh|s^bs.%--__uh - okay .__c1
__s^co__so it's uh broader vocabulary .__c1
__s^cc__yeah .__c3
__sj^bu__then - and ==__c1
__s^aa__okay .__c1
__s__so i think what i'm - what i saw in your smaller chart that i was thinking of was - was there were some numbers i saw i think that included these multiple languages .__c1
__s__and the it - and i was seeing that it got worse .__c1
__s__i - i think that was all it was .__c1
__s__you had some very limited results that - at that point .__c1
__s__yeah .__c3
__s^2__which showed having in these these other languages .__c1
__s^bk__in fact it might have been just this last category having two languages broad that were - where - where english was removed .__c1
__fh|s__so that was cross language .__c1
__s__and the - and the result was quite poor .__c1
__qy^d^g^rt__what i - we hadn't seen yet was that if you added in the english it's still poor .__c1
__s^aa__yeah .__c3
__s^bk__uh - um - now what's the noise condition um - of the training data ?__c1
__sj__still poor .__c3
__s__well i think this is what you were explaining .__c1
__h__the noise condition is the same .__c1
__s__it's the same .__c1
__%-__uh - aurora noises uh - in all these cases for the training .__c1
__fg|s^co__yeah yeah .__c3
__fh|s__so there's not a statistical - sta- - a strong st- - statistically different noise characteristic between uh - the training and test .__c1
__s__no these are the s- - s- - s- - same noises .__c3
__s.%--__and yet we're seeing some kind of effect ==__c1
__s__yeah .__c3
__s__at least - at least for the first - for the well matched ==__c3
__s__well matched condition .__c9
__fh|s.%--__yeah .__c3
__fh|s__right .__c1
__qy^d^f^g^rt__so there's some kind of a - a - an effect from having these - uh - this broader coverage .__c1
__s^df__um ==__c1
__qw^rt__now i guess what we should try doing with this is try testing these on u- - this same sort of thing .__c1
__qy^d^g^rt__on ==__c1
__s__you probably must have this lined up to do .__c1
__s^aa__to - try the same - t- - with the exact same training do testing on the other languages .__c1
__s__huh .__c3
__s.%--__on - on um ==__c1
__s__so - um ==__c1
__s.%--__oh i - well wait a minute .__c1
__qy^d^g^rt__you have this here for the italian .__c1
__s^aa__that's right .__c1
__s^ar__yeah .__c3
__%--__okay .__c1
__s.%--__so - so ==__c1
__s__yeah so for the italian the results are stranger .__c3
__s.%--__uh - um - huh ==__c3
__b__so what appears is that perhaps spanish is not very close to italian .__c3
__s__because uh - well when using the - the network trained only on spanish it's - the error rate is almost uh - twice the baseline error rate .__c3
__b__uhhuh .__c1
__b__huh uh .__c3
__s__well i mean - let's see .__c1
__s__is there any difference in ?==__c1
__s__so it's in the uh ==__c1
__s__so you're saying that when you train on english and uh - and - and test on ==__c1
__fh|s__yeah .__c3
__s__no you don't have training on english testing .__c1
__s.%--__there - there is - another difference is that the noise - the noises are different .__c3
__s__well for - for the italian part i mean .__c3
__s__in - in what ?__c1
__s__the uh - the um - networks are trained with noise from aurora - t i digits .__c3
__sj^bsc__aurora two .__c4
__s__huh .__c3
__s__yeah .__c3
__sj^ba__and the noise is different in th- ==__c1
__fg__and perhaps the noise are quite different from the noises in the speech that italian .__c3
__fg__do we have any um - test sets uh - in any other language that um - have the same noise as in the aurora ?__c1
__s^nd__and ==__c3
__s__huh no .__c4
__b__no .__c3
__b__can i ask something real quick ?__c0
__s.%--__in - in the upper part - in the english stuff it looks like the very best number is sixty point nine .__c0
__qy^d^rt__and that's in the uh - the third section in the upper part under p l p j rasta .__c0
__s^2.%-__sort of the middle column .__c0
__s^aa__yeah .__c3
__s^bk__i- - is that a noisy condition ?__c0
__b__yeah .__c3
__s.%--__so that's matched training .__c0
__s__is that what that is ?__c0
__s__it's ==__c3
__b__no .__c3
__s^cs__the third part .__c3
__s^cs__so it's uh - highly mismatched .__c3
__fh|s^e__so training and test noise are different .__c3
__b__so why do you get your best number in ?==__c0
__s__wouldn't you get your best number in the clean case ?__c0
__b__well it's relative to the um - baseline mismatching .__c2
__fh|s.%--__yeah .__c3
__s^bu.%-__uh !__c0
__s^2__yeah yeah .__c3
__fg|s__okay .__c0
__b__so these are not ==__c0
__fh|qh__okay .__c0
__qh__all right i see .__c0
__b__yeah .__c2
__s^aa__okay .__c0
__s^aa__and then - so in the - in the um - in the non mismatched clean case your best one was under m f c c ?__c0
__sj__that sixty one point four ?__c0
__s^bk__yeah | but it's not a clean case .__c3
__s__it's a noisy case .__c3
__sj__but uh - training and test noises are the same .__c3
__sj__oh so this upper third ==__c0
__%--__so yeah .__c3
__sj.%--__uh - that's still noisy ?__c0
__s^bk__yeah .__c3
__s.%--__uh - okay .__c0
__s__so it's always noisy basically .__c3
__fh__uhhuh .__c0
__s^t^tc__and well the ==__c3
__fh|s.%--__i see .__c0
__s^2__huh .__c3
__s^m__okay .__c1
__b__um - so uh ==__c1
__s__i think this will take some looking at thinking about .__c1
__s__but what is uh - what is currently running that's uh - i- - that - just filling in the holes here ?__c1
__s.%-__or - or pretty much ?__c1
__s^bu.%--__uh - no we don't plan to fill the holes .__c3
__qy^bu^rt__but actually there is something important .__c3
__s^aa__okay .__c1
__s^am__is that um - we made a lot of assumption concerning the online normalization .__c3
__fh|s__and we just noticed uh - recently that uh - the approach that we were using was not uh - leading to very good results when we used the straight features to h t k .__c3
__fh|s.%--__um - huh .__c3
__s__so basically d- - if you look at the - at the left of the table the first uh - row with eighty six one hundred and forty three and seventy five ==__c3
__fh__these are the results we obtained for italian uh - with straight huh p l p features using online normalization .__c3
__s^aa__uhhuh .__c1
__s^bk__huh - and the huh - what's in the table just at the left of the p l p twelve online normalization column .__c3
__s^bu__so the numbers seventy nine fifty four and uh - forty two are the results obtained by uh - pratibha with uh his online normalization - it's uh - error her online normalization approach .__c3
__%-__where is that ?__c0
__s^aa__seventy nine fifty ?__c0
__fg__uh - | it's just sort of sitting right on the uh - the column line .__c1
__s^e__fifty one ?__c4
__s^aa__so ==__c3
__s__this ==__c4
__b__uh - yeah .__c1
__sj^ba__oh i see okay .__c0
__s__just - uh ==__c3
__s^cc__yeah .__c3
__s__so these are the results of o g i with online normalization .__c3
__fh__and straight features to h t k .__c3
__s.%-__and the previous result eighty six and so on are with our features straight to h t k .__c3
__sj__yes .__c1
__sj__yes .__c1
__sj^e__so what we see that is there is - that um - uh - the way we were doing this was not correct .__c3
__s^bk__but still the networks are very good .__c3
__s^bk__when we use the networks our number are better that uh - pratibha results .__c3
__qy^d^g__we improve .__c4
__s^cs.%--__so do you know what was wrong with the online normalization or ?__c1
__fh__yeah .__c3
__s__there were diff- - there were different things .__c3
__s.%--__and basically the first thing is the huh - alpha uh - value .__c3
__fh__so the recursion uh - part ==__c3
__s__um - i used point five per cent which was the default value in the - in the programs here .__c3
__sj^nd.%--__and pratibha used five per cent .__c3
__qy^d^g__uh - uhhuh .__c1
__sj.%-__so it adapts more quickly .__c3
__s^aa__yes .__c1
__s^bk__yeah .__c1
__sj^cs.%--__um - but ==__c3
__sj__yeah .__c3
__sj__i assume that this was not important because uh previous - results from - from dan and show that basically the both - both values g- - give the same - same uh - results .__c3
__s^2__it was true on uh - as t i digits but it's not true on italian .__c3
__b__uhhuh .__c1
__%--__uh second thing is the initialization of the stuff .__c3
__s__actually uh - what we were doing is to start the recursion from the beginning of the utterance .__c3
__s^aa__and using initial values that are the global mean and variances measured across the whole database .__c3
__sj__right .__c1
__s^cs__right .__c1
__s__and pratibha did something different is .__c3
__fh|s__that he - uh - she initialed the um values of the mean and variance by computing this on the twenty five first frames of each utterance .__c3
__fh|s^cs__huh - there were other minor differences .__c3
__s^e__the fact that she used fifteen dissities instead - s- - instead of thirteen .__c3
__s__and that she used c zero instead of log energy .__c3
__qy^d^g^rt__uh - but the main difference is - differences concerns the recursion .__c3
__s^e__so uh - i changed the code .__c3
__s.%-__uh - and now we have a baseline that's similar to the o g i baseline .__c3
__s^aa__okay .__c1
__s^rt__we - it - it's slightly uh - different .__c3
__fh|s^rt__because i don't exactly initialize the same way she does .__c3
__s__actually i start - huh i don't wait to a - fifteen twenty five twenty five frames before computing a mean and the variance to - e- - to - to start the recursion .__c3
__s.%--__uhhuh .__c2
__s^cs__yeah .__c1
__s^cs^rt__i - i use the online scheme .__c3
__s__and only start the re- - recursion after the twenty five - twenty fifth frame .__c3
__%--__but - well it's similar .__c3
__s.%--__so uh - i retrained the networks with these .__c3
__s__well the - the - the networks are retaining with these new features .__c3
__s__uhhuh .__c1
__s__and ==__c3
__s__yeah .__c3
__qy^rt__okay .__c1
__b__so basically what i expect is that these numbers will a little bit go down .__c3
__qy^rt__but perhaps not - not so much .__c3
__b__right .__c1
__s__because i think the neural networks learn perhaps to ==__c3
__s^cs__to even if the features are not normalized it - it will learn how to normalize .__c3
__sj^rt__right .__c1
__%-__and uh ==__c3
__fg__okay | but i think that given the pressure of time we probably want to draw because of that especially we want to draw some conclusions from this .__c1
__sj__do some reductions in what we're looking at .__c1
__s__yeah .__c3
__s^aa__and make some strong decisions for what we're going to do testing on before next week .__c1
__s^bk__yeah i - i'd ==__c3
__sj^ba__so do you - are you - w- - did you have something going on - on the side with uh multi band or on - on this ?__c1
__h|s^aa__no .__c3
__s.%-__or ?==__c1
__s^m^na__i - we plan to start this .__c3
__s^e__uh - so - act- - actually we have discussed uh - @reject@ um - these .__c3
__fh|s__what we could do more as a - as a research .__c3
__sj__and - and we were thinking perhaps that uh - the way we use the tandem is not ==__c3
__sj.%-__uh well there is basically perhaps a flaw in the - in the - the stuff .__c3
__s^m__because we trained the networks ==__c3
__s__if we trained the networks on the - on a language and a t- - or a specific task ==__c3
__s^f__uhhuh .__c1
__s__um - what we ask is - to the network - is to put the bound - bound- - the decision boundaries somewhere in the space .__c3
__s__huh .__c1
__fh|s__and - uh - huh - and ask the network to put one - at one side of the - for - for a particular phoneme at one side of the boundary - decision boundary .__c3
__s:s^fe__and one for another phoneme at the other side .__c3
__s__and so there is kind of reduction of the information there that's not correct .__c3
__qy^d^g__because if we change task and if the phonemes are not in the same context in the new task obviously the decision boundaries are not - should not be at the same place .__c3
__s__but the way the feature gives - the - the way the network gives the features is that it reduce completely the - it removes completely the information - a lot of information from the - the features by uh - uh placing the decision boundaries at optimal places for one kind of data .__c3
__qy^d^g__i di- ==__c1
__s__but this is not the case for another kind of data .__c3
__b__it's a trade off .__c1
__sj^ba__so ==__c3
__b__right ?__c1
__s.%--__any- - anyway go ahead .__c1
__h|s__yeah .__c3
__s.%--__so uh - we were thinking about is perhaps um - one way to solve this problem is increase the number of outputs of the neural networks .__c3
__fh__doing something like um - um - phonemes within context and ==__c3
__sj^cc__well basically context dependent phonemes .__c3
__s^bk^rt__maybe .__c1
__s__i mean - i - i think you could make the same argument .__c1
__s__it'd be just as legitimate for hybrid systems as well .__c1
__s__yeah | but we know that ==__c3
__s^cs__right ?__c1
__s^bk__and in fact th- - things get better with context dependent versions .__c1
__fg__right ?__c1
__%--__ye- - yeah | but here it's something different .__c3
__s__we want to have features .__c3
__s__uh - well um ==__c3
__h__yeah .__c1
__%--__yeah | but it's still true that what you're doing is ==__c1
__s^cc__you're ignoring ==__c1
__s__you're - you're coming up with something to represent whether it's a distribution - part of the probability distribution or features .__c1
__fh|s.%--__you're coming up with a set of variables that are representing uh - things that vary w- - over context .__c1
__s__uhhuh .__c3
__fh|s__uh - and you're putting it all together .__c1
__s__ignoring the differences in context .__c1
__s__that - that's true for the hybrid system .__c1
__s__it's true for a tandem system .__c1
__s.%__so for that reason when you - in - in - in a hybrid system when you incorporate context one way or another you do get better scores .__c1
__s.%-__yeah .__c3
__qw.%--__okay .__c1
__b__but i - it's - it's a big deal to get that .__c1
__s__i - i'm i'm sort of ==__c1
__qy^d^g__and once you - the other thing is that once you represent - start representing more and more context it is uh - much more um - specific to a particular task in language .__c1
__s.%--__so uh - the - the acoustics associated with uh a particular context ==__c1
__s__for instance you may have some kinds of contexts that will never occur in one language and will occur frequently in the other .__c1
__fh__so the qu- ==__c1
__b__the issue of getting enough training for a particular kind of context becomes harder .__c1
__fh__we already actually don't have a huge amount of training data .__c1
__s^cs.%--__um ==__c1
__qy:qw^rt__yeah | but - huh i mean - the ==__c3
__%--__the way we - we do it now is that we have a neural network .__c3
__s__and basically the net- - network is trained almost to give binary decisions .__c3
__s:qw__right .__c1
__s.%--:qw.%--__and uh - binary decisions about phonemes .__c3
__b__nnn - uh - it's ==__c3
__qy.%-__almost .__c1
__s__but i mean - it - it - it does give a distribution .__c1
__b__yeah .__c3
__s__it's - and - and it is true that if there's two phones that are very similar that uh - the - i- - it may prefer one but it will give a reasonably high value to the other too .__c1
__sj.%--__yeah .__c3
__fg__yeah sure .__c3
__%__but ==__c3
__%--__uh - | so basically it's almost binary decisions .__c3
__fg|sj__and um - the idea of using more classes is to get something that's less binary decisions .__c3
__s^am__oh no .__c1
__fh__but it would still be even more of a binary decision .__c1
__s__it - it'd be even more of one .__c1
__s__because then you would say that in - that this phone in this context is a one but the same phone in a slightly different context is a zero .__c1
__b^rt__but - yeah but ==__c3
__s^aa__that would be even - even more distinct of a binary decision .__c1
__h__i actually would have thought you'd want to go the other way and have fewer classes .__c1
__s^cc__yeah but if ==__c3
__s^no__uh - i mean - for instance the - the thing i was arguing for before but again which i don't think we have time to try is something in which you would modify the code so you could train to have several outputs on and use articulatory features .__c1
__b__huh .__c3
__%-__uhhuh .__c3
__s^ar__because then that would - that would go - that would be much broader and cover many different situations .__c1
__s^ar|s.%-__but if you go to very very fine categories it's very binary .__c1
__s^m__huh .__c3
__s__yeah but i think ==__c3
__s__yeah perhaps you're right .__c3
__s^e__but you have more classes .__c3
__s__so you - you have more information in your features .__c3
__s__so um - you have more information in the uh - posterior spectrum .__c3
__s__uhhuh .__c1
__s^2.%-__true .__c1
__s__posteriors vector ==__c3
__s__um ==__c3
__sj__which means that ==__c3
__fh|s__but still the information is relevant .__c3
__fh|s__uhhuh .__c1
__fh|s__because it's - it's information that helps to discriminate .__c3
__s__uhhuh .__c1
__s__if it's possible to be able to discriminate amongst uh - among the phonemes in context .__c3
__s__well it's ==__c1
__s.%-__but the ==__c3
__s^am__it's - it's an interesting thought .__c1
__s^am^r__i mean - we - we could disagree about it at length .__c1
__s.%--__huh .__c3
__s.%--__huh .__c3
__s.%--__but the - the real thing is if you're interested in it you'll probably try it .__c1
__s__and - and we'll see .__c1
__qy^d^f^g__but - but what i'm more concerned with now as an operational level is uh - you know ==__c1
__s__huh .__c3
__s__what do we do in four or five days ?__c1
__s__uh - and so we have to be concerned with ==__c1
__qy^d^f^g__are we going to look at any combinations of things ?__c1
__%--__you know - once the nets get retrained so you have this problem out of it .__c1
__s.%--__huh .__c3
__s__um - | are we going to look at multi band ?__c1
__s__are we going to look at combinations of things ?__c1
__s^e__uh - what questions are we going to ask ?__c1
__b__uh - now that ==__c1
__s^tc__i mean we should probably turn shortly to this o g i note .__c1
__b__um - how are we going to combine with what they've been focusing on ?__c1
__fh__uh - uh - | we haven't been doing any of the l d a rasta sort of thing .__c1
__sj__uhhuh .__c3
__s^df__and they - although they don't talk about it in this note .__c1
__fh__um - there's um - the issue of the um - new mu law business uh - versus the logarithm .__c1
__s^ng.%-__uhhuh .__c3
__s^aap__um - so ==__c1
__s^aa__so what - i- - what is going on right now ?__c1
__s__what's ?==__c1
__s__right .__c1
__s^aa__you've got nets retraining .__c1
__b__are there - is there - are there any h t k trainings testings going on ?__c1
__s^t1__n- ==__c3
__s^no__i - i - i'm trying the h t k with uh - p l p twelve online delta delta and m s g filter together .__c4
__s__the combination .__c1
__h__i see .__c1
__s^fe__the combination yeah .__c4
__s__but i haven't result at this moment .__c4
__s__m s g and - and p l p .__c1
__s__yeah .__c4
__s__yeah .__c4
__b__and is this with the revised online normalization ?__c1
__b__ye- - uh - with the old - old - older .__c4
__fg|qy^rt__yeah .__c3
__sj__old one .__c1
__s__yeah .__c4
__s__so it's using all the nets for that .__c1
__s.%-__but again we have the hope that it - we have the hope that it - maybe it's not making too much difference .__c1
__s^aa__yeah .__c4
__s.%-__but we can ==__c4
__s^fe__know soon .__c4
__sj__maybe .__c4
__b__but - but ==__c1
__s__yeah .__c1
__h__i don't know .__c4
__fh|s.%-__yeah .__c3
__sj^ba__uh - okay .__c1
__sj^ba^rt__uh - so there is this combination .__c3
__qh__yeah .__c3
__fh|s^rt__working on combination obviously .__c3
__s^aa__uhhuh .__c4
__fh|s.%-__um - i will start work on multi band .__c3
__fg__and we plan to work also on the idea of using both features and net outputs .__c3
__fh|s__yup .__c4
__sj^ba__um - | and we think that with this approach perhaps we could reduce the number of outputs of the neural network .__c3
__b__um - so get simpler networks .__c3
__b__because we still have the features .__c3
__s.%--__so we have um - come up with um - different kind of broad phonetic categories .__c3
__s__and we have - basically we have three types of broad phonetic classes .__c3
__s__well something using place of articulation .__c3
__qy^d^g^rt__which - which leads to nine i think broad classes .__c3
__sj.%--__uh - another which is based on manner .__c3
__s^aa__which is - is also something like nine classes .__c3
__fh|s^rt__and then something that combine both .__c3
__s__and we have twenty - f- - twenty five ?__c3
__qy^d^g^rt__twenty seven .__c9
__s__twenty seven broad classes .__c3
__s__so like - uh ==__c3
__fh__oh i don't know .__c3
__b__like back vowels front vowels ==__c3
__s__so what you do ==__c1
__sj__um - for the moments we do not - don't have nets .__c3
__fh|s^df__um - um i just want to understand .__c1
__b__so you have two net or three nets ?__c1
__fh__was this ?==__c1
__s^bk__how many - how many nets do you have ?__c1
__s__i mean - it's just - were we just changing the labels to retrain nets with fewer out- - outputs .__c3
__b__no nets .__c1
__s__begin to work in this .__c4
__b__we are @reject@ ==__c4
